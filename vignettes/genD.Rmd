---
title: "Bates and Watts D Matrix"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Bates and Watts D Matrix}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

This vignettes describes what the `genD` function is calculating.

The derivatives are calculated numerically using _Richardson improvement_. 
This method calculates a numerical approximation of the first and second derivatives of a function `func` at a given point `x`.
For a scalar valued function these correspond to the gradient vector and Hessian matrix. 
For a vector valued function the first derivative is the Jacobian matrix.

The default arguments passed to `genD` are: `method.args = list(eps = 1e-4, d = 0.0001, zero.tol = sqrt(.Machine$double.eps / 7e-7), r = 4, v = 2)`.

A simple approximation to the first order derivative with respect to $x_i$ is
$$
f'_{i}(x) = \frac{f(x_{1}, \dots, x_{i} + d, \dots ,x_{n}) - f(x_{1}, \dots , x_{i} - d, \dots, x_{n})}{2d}
$$

A simple approximation to the second order derivative with respect to $x_i$ is
$$
f''_{i}(x) = \frac{f(x_{1}, \dots ,x_{i} + d, \dots , x_{n}) - 2f(x_{1}, \dots, x_{n}) + f(x_{1}, \dots, x_{i} - d, \dots, x_{n})}{d^2}
$$

The second order derivative with respect to $x_i$, $x_j$ is
$$
f''_{i,j}(x) = \frac{f(x_{1}, \dots, x_{i} + d, \dots, x_{j} + d, \dots, x_{n}) - 2f(x_{1}, \dots, x_{n}) + f(x_{1}, \dots, x_{i} - d, \dots, x_{j} - d, \dots, x_{n})}{2d^2 - [f''_{i}(x) + f''_{j}(x)]/2}
$$

Richardson's extrapolation is based on these formulae, with $d$ being reduced in the extrapolation iterations. 
In the code, $d$ is scaled to accommodate parameters of different magnitudes.

`genD` does $1 + r(N^2 + N)$ evaluations of the function `func`, where $N$ is the length of `x`.

